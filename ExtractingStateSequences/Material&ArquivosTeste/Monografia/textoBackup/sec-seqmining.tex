\section{Sequential pattern mining}
\label{sec-seqmining}

Sequential pattern mining is a topic in data mining that discovers frequent subsequences as patterns in a sequence
database \cite{Nizar}. Each sequence in a sequence database is called data-sequence and contains typically an ID and transactions ordered generally by time, where each transaction is a set of items.

The problem is to find all sequential patterns with a user specified minimum support, where the support of a sequential pattern is the percentage of data-sequences that contain the pattern \cite{Rakesh}. In other words, if a user inputs a percentage $p$ and a sequence database $D$ is given, then the mining will return the set of patterns that appear in at least $p\%$ of the data-sequences. The formal definition can be found in \cite{Nizar} and in \cite{Pei}.

There are several applications for the problem, such as analysis of customer behaviour, purchase patterns in a store and study of DNA sequences. In section \ref{webusage}, a practical example is described based on \cite{Nizar}.

\textbf{Notation}

A data-sequence $S$ that has ID $T$ and $n \geq 1$ ordered transactions $t_1,t_2,...,t_n$ is denoted by $S = [T <t_1,t_2,...,t_n>]$. 

Each transaction $t_i$ that is a set of $m \geq 1$ items $l_{i_1},...,l_{i_m}$ and is denoted by $t_i = (l_{i_1},...,l_{i_m})$. Thus, $S = [T <(l_{1_1},...,l_{1_m}), ... ,(l_{n_1},...,l_{n_m})>]$. 

To simplify the notation in the case in which each transaction contains only one item, we can avoid the parenthesis, for example: if $t_i = (l_i)$ for all $ 1 \leq i \leq n$ than it is possible to write $S = [T <l_1l_2...l_n>]$.


\subsection{An example: Web usage mining}
\label{webusage}

Web usage mining, also known as web log mining, is an important application of sequential pattern mining. It is concerned with finding frequent patterns related to user navigation from the information presented in web system's log. Considering that a user is able to access only one page at a time, the data-sequences would only have transitions with a single event each.

In an e-commerce application, for instance, we can have the set of items $I = {a, b, c, d, e, f}$ representing products that can be purchased. The occurrence of one of these items in a transaction means that a user accessed the page of such item.

Suppose the sequence database contains the following data-sequences extracted from the log: $[T1 <abdac>], [T2 <eaebcac>], [T3 <babfaec>]$ and $[T4 <abfac>]$. In this case, the analysis of the first transaction allows us to conclude that user $T1$ accessed the pages of products $a,b,d,a$ and $c$ in this order. By applying the web usage mining technique with support of 90\%, a manager would notice that $abac$ is a frequent pattern, indicating that $90\%$ of the users who visited product $a$ then visit $b$, then return to $a$ and later visit $c$. Hence, an offer could be placed in product $a$, which is visited many times in sequence, to increase the sales of other products. 


\subsection{Sequential pattern mining algorithms}

There are several algorithms to perform the sequential pattern mining, but they differ in two aspects\cite{Nizar}:

\begin{itemize}

\item The way in which candidate sequences are generated and stored. The goal is to reduce the amount of candidates created as well as decrease I/O costs.

\item The way in which support is counted and how candidate sequences are tested for frequency. The goal is to eliminate data structures used for support or counting purposes only.

\end{itemize}

Considering these topics, algorithms for sequential pattern mining can be divided in two categories: apriori-based and pattern-growth

\subsubsection{Apriori-based algorithms}

These algorithms mainly rely on the property that states that if a sequence $s$ is infrequent, then any other sequence that contains $s$ is also infrequent. The $GSP$\cite{Rakesh} algorithm is an example in this category.

Apriori-based algorithms use the \textit{generate-and-test} method to obtain the candidate patterns: the pattern is grown one item at a time and tested against the minimum support. By taking this approach, they have to maintain the support counting for each candidate and test it at each iteration of the algorithm.

In general, algorithms in this category generate an explosive number of candidate sequences, consuming a lot of memory. \textit{GSP}, for instance, generates a combinatorially explosive number of candidates when mining long sequential patterns. This is the case of a DNA analysis application, in which many patterns are long chains.

In addition, since they need to check at each iteration for the support count, multiple scans of the database are performed, which requires a lot of processing time and the I/O is very costly. In general, to find a sequential pattern of length $l$, the apriori-based method must scan the database at least $l$ times. For problems in which long patterns exist, this feature increases the cost of the application.

\subsubsection{Pattern-growth algorithms}

Pattern-growth algorithms try to use a certain data structure to prune candidates early in the mining process. Besides that, the search space is partitioned for efficient memory management. \textit{PrefixSpan}\cite{Pei} is an example of such an algorithm and it is used in our implementation due to its adequately to our problem.

The general idea behind \textit{PrefixSpan} is as follows: Instead of repeatedly scanning the entire database, generating and testing large sets of candidate sequences, one can recursively project a sequence database into a set of smaller databases associated with the set of patterns mined so far and, then, mine locally frequent patterns in each projected database\cite{Pei}.

To reduce the projections size and the number of access, \textit{PrefixSpan}, sorts the items inside each transaction and creates the projected databases based on patterns' prefixes. In order to do so, the algorithm assumes that the order of items in a transaction is irrelevant, and only the order of the whole transactions matters to the problem.

Differently from apriori-based algorithms, \textit{PrefixSpan} does not generate or test candidate sequences. Patterns are grown from the shorter ones and projected databases keep shrinking. This is relevant in practice because, in general only a small set of sequential patterns grows long in a database and, thus, the number of sequences in a projected database usually reduces when prefix grows.

In the worst case, \textit{PrefixSpan} constructs a projected database for every sequential pattern. With the intention to reduce the number of projected databases and improve the performance of the algorithm, \cite{Pei} proposes a technique called \textit{pseudo partition} that may reduce the number and size of the projected databases.

